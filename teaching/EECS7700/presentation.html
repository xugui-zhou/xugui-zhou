<!DOCTYPE html>
<html>
<head>
    <title>EECS 7700  - Schedule</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" title="style1">
    <!-- <link rel="icon" type="image/ico" href="images/favicon.ico"> -->
    <!-- <link href='https://fonts.googleapis.com/css?family=Open+Sans:400' rel='stylesheet' type='text/css'> -->
</head>
<body>

    <ul id="nav">
	    <li><a href="index.html">Home</a></li>
	    <li><a href="schedule.html">Schedule</a></li>
	    <li><a style="color:#232C2D; background:#FFFFFF" href="presentation.html">Presentation</a></li>
	</ul>
    

    <div id="all">
        

        <div id="intro">
            <table width = "100%">
                <!-- <tr>
                    <td>
                        <p align="left" style="font-size:22px">
                            <b>EE/CSC 7700 - Fall 2024 Paper Presentations<br></b>
                        </p>
                        
                        
                    </td>
                </tr> -->

                <tr>
                    <td>
                        <p align="left" style="font-size:18px">
                            <b>Week 3: Machine Learning Applications<br></b>
                        </p>
                        <p>
                            <b>Reading tasks </b> <br>
                            
                            Deep Residual Learning for Image Recognition [<a href="https://arxiv.org/abs/1512.03385" target="_blank"> Link </a>]<br>
                            Attention Is All You Need [<a href="https://arxiv.org/abs/1706.03762" target="_blank"> Link </a>]<br>
                            Privacy Auditing with One (1) Training Run [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9a6f6e0d6781d1cb8689192408946d73-Paper-Conference.pdf" target="_blank"> Link </a>]<br>
                        </p>

                        <p>
                            <b>Blog Post 1: ResNet </b> <br>
                            As the number of layers of neural networks increases, the problems of overfitting, gradient vanishing, and gradient explosion often occur, so this article came into being. In this paper, the concept of deep residual networks (ResNets) is proposed. By introducing "shortcut connections," this study solves the problem of gradient vanishing in deep network training and has an important impact on the field of deep learning. The method of the paper explicitly redefines the network layers as learning residual functions relative to the inputs. By learning residuals, the network can be optimized more easily and can train deeper models more efficiently. Therefore, this method can help solve the performance degradation problem that may occur when the network layer increases. In addition, the article displays the experimental part. The model shows significant improvements in handling large-scale visual recognition tasks like ImageNet and CIFAR-10. The application of deep residual networks in major visual recognition competitions like ILSVRC and COCO 2015 further proves their power and wide applicability.
                            [<a href="./presentations/p1/final.html" target="_blank">Read more ...</a>]<br>
                        </p>
                        <!-- <p align="right" style="font-size:18px">
                         <a href="./presentations/p1/final.html" target="_blank"> Read more ... </a><br>
                        </p> -->
                        
                        
                    </td>
                </tr>

            </table>
        </div>
        <br>
        <br>
    </div>


    <!-- <hr> -->
    </br>

  

</body>
</html>